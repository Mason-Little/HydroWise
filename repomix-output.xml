This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
apps/
  web/
    src/
      api/
        chat.ts
        document.ts
        rag.ts
      components/
        ui/
          HydroButton.tsx
          HydroInput.tsx
          HydroInputContainer.tsx
          HydroMessage.tsx
        HydroChat.tsx
        HydroSidebar.tsx
        UploadDocumentModel.tsx
      hooks/
        useChat.ts
        useDocument.ts
        useInitEngine.ts
      store/
        chatStore.ts
      App.tsx
      main.tsx
      theme.ts
    index.html
    package.json
    tsconfig.app.json
    tsconfig.json
    tsconfig.node.json
    vite.config.ts
packages/
  core/
    src/
      document/
        docx/
          parse-docx.ts
        image/
          parse-image.ts
        pdf/
          parse-pdf.ts
        pptx/
          parse-pptx.ts
        embeddings.ts
        index.ts
        parse.ts
      utils/
        chunk.ts
        index.ts
      index.ts
    package.json
    tsconfig.json
  db/
    src/
      desktop.ts
      index.ts
      schema.ts
      web.ts
    drizzle.config.ts
    package.json
    tsconfig.json
  entities/
    src/
      Chat.ts
      Document.ts
      index.ts
      Message.ts
    package.json
    tsconfig.json
  llm-client/
    src/
      config/
        ocr-correction-prompt.ts
      desktop/
        init/
          chat.ts
          embedding.ts
          image.ts
        run/
          chat.ts
          embeddings.ts
          image.ts
          ocr.ts
      web/
        init/
          chat.ts
          embeddings.ts
          image.ts
          logging.ts
        run/
          chat.ts
          embeddings.ts
          image.ts
          ocr.ts
      client.ts
      config.ts
      index.ts
    package.json
    tsconfig.json
    vite.config.ts
services/
  api/
    src/
      routes/
        chat/
          chat.ts
          index.ts
        document/
          document.ts
          index.ts
        rag/
          index.ts
          rag.ts
      config.ts
      index.ts
    package.json
    README.md
    tsconfig.json
.gitignore
biome.json
package.json
Readme.md
turbo.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="apps/web/src/api/chat.ts">
import type { Chat } from "@hydrowise/entities";

const BASE_URL = import.meta.env.VITE_SERVER_BASE_URL;

// TODO: replace with real auth user id
const USER_ID = "1";

const withUserHeaders = (headers: HeadersInit = {}) => ({
  ...headers,
  userId: USER_ID,
});

const jsonHeaders = withUserHeaders({ "Content-Type": "application/json" });

export const getChats = async (): Promise<Chat[]> => {
  const response = await fetch(`${BASE_URL}/chat`, {
    headers: withUserHeaders(),
  });
  if (!response.ok) {
    throw new Error("Failed to fetch chats");
  }
  return response.json();
};

export const getChat = async (chatId: string) => {
  const response = await fetch(`${BASE_URL}/chat/${chatId}`, {
    headers: withUserHeaders(),
  });
  if (!response.ok) {
    throw new Error("Failed to fetch chat");
  }
  return response.json();
};

export const createChat = async (name?: string) => {
  const response = await fetch(`${BASE_URL}/chat`, {
    method: "POST",
    headers: jsonHeaders,
    body: JSON.stringify({ name }),
  });
  if (!response.ok) {
    throw new Error("Failed to create chat");
  }
  return response.json();
};

export const deleteChat = async (chatId: string) => {
  const response = await fetch(`${BASE_URL}/chat/${chatId}`, {
    method: "DELETE",
    headers: withUserHeaders(),
  });
  if (!response.ok) {
    throw new Error("Failed to delete chat");
  }
  return response.json();
};

export const getMessages = async (chatId: string) => {
  const response = await fetch(`${BASE_URL}/chat/${chatId}/messages`, {
    headers: withUserHeaders(),
  });
  if (!response.ok) {
    throw new Error("Failed to fetch messages");
  }
  return response.json();
};

export const appendMessage = async (
  chatId: string,
  role: string,
  content: string,
) => {
  const response = await fetch(`${BASE_URL}/chat/${chatId}/messages`, {
    method: "POST",
    headers: jsonHeaders,
    body: JSON.stringify({ role, content }),
  });
  if (!response.ok) {
    throw new Error("Failed to append message");
  }
  return response.json();
};
</file>

<file path="apps/web/src/api/document.ts">
import type { CreateDocumentResponse, DocumentMeta } from "@hydrowise/entities";

const BASE_URL = import.meta.env.VITE_SERVER_BASE_URL;

const USER_ID = "1";

const withUserHeaders = (headers: HeadersInit = {}) => ({
  ...headers,
  userId: USER_ID,
});

const jsonHeaders = withUserHeaders({ "Content-Type": "application/json" });

export const createDocument = async (
  payload: DocumentMeta,
): Promise<CreateDocumentResponse> => {
  const response = await fetch(`${BASE_URL}/document`, {
    method: "POST",
    headers: jsonHeaders,
    body: JSON.stringify(payload),
  });

  if (!response.ok) {
    throw new Error("Failed to create document");
  }

  return response.json();
};
</file>

<file path="apps/web/src/api/rag.ts">
import { sendEmbeddings } from "@hydrowise/llm-client";

const BASE_URL = import.meta.env.VITE_SERVER_BASE_URL;

export const getRagApi = async (userEmbedding: string) => {
  const embeddings = await sendEmbeddings([userEmbedding]);

  const response = await fetch(`${BASE_URL}/rag/retrieve-context`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      userId: "1",
    },
    body: JSON.stringify({ embedding: embeddings[0] }),
  });
  if (!response.ok) {
    throw new Error("Failed to retrieve RAG context");
  }

  const data: { data: { content: string }[] } = await response.json();
  return data.data.map((item) => item.content).join("\n");
};
</file>

<file path="apps/web/src/hooks/useDocument.ts">
import { parseDocumentMeta } from "@hydrowise/core";
import type { CreateDocumentResponse } from "@hydrowise/entities";
import { useMutation, useQueryClient } from "@tanstack/react-query";
import { createDocument } from "@/api/document";

export interface UseDocumentOptions {
  /** Callback when upload succeeds */
  onSuccess?: (response: CreateDocumentResponse) => void;
  /** Callback when upload fails */
  onError?: (error: Error) => void;
}

export const useDocument = (options: UseDocumentOptions = {}) => {
  const queryClient = useQueryClient();

  const {
    mutateAsync: uploadDocument,
    error,
    isPending,
    isSuccess,
    isError,
    reset,
  } = useMutation({
    mutationFn: async (file: File) => {
      const meta = await parseDocumentMeta(file);
      return createDocument(meta);
    },
    onSuccess: (response) => {
      // Invalidate any document-related queries
      queryClient.invalidateQueries({ queryKey: ["documents"] });
      options.onSuccess?.(response);
    },
    onError: (err) => {
      options.onError?.(err instanceof Error ? err : new Error(String(err)));
    },
  });

  return {
    uploadDocument,
    error,
    isPending,
    isSuccess,
    isError,
    reset,
  };
};
</file>

<file path="apps/web/src/theme.ts">
import { createTheme } from "@mui/material/styles";

const theme = createTheme();

export default theme;
</file>

<file path="apps/web/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>HydroWise</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="apps/web/tsconfig.json">
{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}
</file>

<file path="apps/web/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="packages/core/src/document/image/parse-image.ts">
import { postprocessOcrText, processImage } from "@hydrowise/llm-client";
import { heicTo, isHeic } from "heic-to";

const convertHeicToPng = async (file: File): Promise<File> => {
  const converted = await heicTo({
    blob: file,
    type: "image/png",
  });

  if (!(converted instanceof Blob)) {
    throw new Error("HEIC conversion did not return a valid image blob");
  }

  const baseName = file.name.replace(/\.[^.]+$/, "") || "image";
  return new File([converted], `${baseName}.png`, { type: "image/png" });
};

const normalizeImageForOcr = async (file: File): Promise<File> => {
  const mimeType = file.type.toLowerCase();
  const isHeicImage = await isHeic(file).catch(() => false);

  if (isHeicImage) {
    return convertHeicToPng(file);
  }

  if (!mimeType || mimeType.startsWith("image/")) {
    return file;
  }

  throw new Error(
    `Unsupported image format for OCR normalization: ${file.type}`,
  );
};

export const parseImage = async (image: File) => {
  try {
    const normalizedImage = await normalizeImageForOcr(image);
    const result = await processImage(normalizedImage);
    const markdown = await postprocessOcrText(result);
    return {
      text: markdown,
      pageCount: 1,
    };
  } catch (error) {
    const details = error instanceof Error ? error.message : String(error);
    const heicHint = details.includes("ERR_LIBHEIF format not supported")
      ? " This HEIC variant is not supported by the local decoder. Export it as JPEG or PNG in Photos and upload that copy."
      : "";
    throw new Error(
      `Failed to decode image for OCR. File: ${image.name}, MIME type: ${image.type || "unknown"}. ${details}${heicHint}`,
    );
  }
};
</file>

<file path="packages/core/src/document/pptx/parse-pptx.ts">
import { pptxToHtml } from "@jvmr/pptx-to-html";
import { gfm } from "@truto/turndown-plugin-gfm";
import TurndownService from "turndown";

const createTurndown = () => {
  const turndown = new TurndownService({
    codeBlockStyle: "fenced",
  });

  turndown.use(gfm);
  turndown.addRule("removeImages", {
    filter: "img",
    replacement: () => "",
  });

  return turndown;
};

export const parsePptx = async (file: File) => {
  const slidesHtml = await pptxToHtml(await file.arrayBuffer());
  const turndown = createTurndown();

  const slidesMarkdown = slidesHtml.map((slideHtml, index) => {
    const markdown = turndown.turndown(slideHtml).trim();
    const heading = `## Slide ${index + 1}`;
    return markdown ? `${heading}\n\n${markdown}` : heading;
  });

  return {
    text: slidesMarkdown.join("\n\n"),
    pageCount: slidesHtml.length || null,
  };
};
</file>

<file path="packages/core/src/document/embeddings.ts">
import type { EmbeddingChunk } from "@hydrowise/entities";
import { sendEmbeddings } from "@hydrowise/llm-client";

export const generateEmbeddings = async (
  chunks: string[],
  onProgress?: (completed: number, total: number) => void,
): Promise<EmbeddingChunk[]> => {
  if (chunks.length === 0) {
    return [];
  }

  const embeddings = await sendEmbeddings(chunks);
  onProgress?.(chunks.length, chunks.length);

  return chunks.map((content, index) => ({
    content,
    embedding: embeddings[index] ?? [],
  }));
};
</file>

<file path="packages/core/src/document/index.ts">
// Main document parsing API

// Embedding generation
export { generateEmbeddings } from "./embeddings";
export { parseDocumentMeta } from "./parse";
</file>

<file path="packages/core/src/utils/index.ts">
export * from "./chunk";
</file>

<file path="packages/core/src/index.ts">
export * from "./document";
export * from "./utils";
</file>

<file path="packages/db/src/desktop.ts">
import { PGlite } from "@electric-sql/pglite";
import { vector } from "@electric-sql/pglite/vector";
import { drizzle } from "drizzle-orm/pglite";
import * as schema from "./schema.js";

export const createDesktopClient = (dbPath: string) => {
  const client = new PGlite(dbPath, { extensions: { vector } });
  return drizzle(client, { schema });
};
</file>

<file path="packages/db/src/web.ts">
import { drizzle } from "drizzle-orm/node-postgres";
import { Pool } from "pg";
import * as schema from "./schema.js";

export const createWebClient = (connectionString: string) => {
  const pool = new Pool({ connectionString });
  return drizzle(pool, { schema });
};
</file>

<file path="packages/db/drizzle.config.ts">
import "dotenv/config";
import { defineConfig } from "drizzle-kit";

export default defineConfig({
  out: "./drizzle",
  schema: "./src/schema.ts",
  dialect: "postgresql",
  dbCredentials: {
    url: process.env["DATABASE_URL"] ?? "",
  },
});
</file>

<file path="packages/entities/src/Chat.ts">
import { z } from "zod";

export const ChatSchema = z.object({
  name: z.string(),
  id: z.string(),
  messageIds: z.array(z.string()),
});

export type Chat = z.infer<typeof ChatSchema>;
</file>

<file path="packages/entities/src/Document.ts">
import { z } from "zod";

export const EmbeddingChunkSchema = z.object({
  content: z.string(),
  embedding: z.array(z.number()),
});

export type EmbeddingChunk = z.infer<typeof EmbeddingChunkSchema>;

export const DocumentMetaSchema = z.object({
  name: z.string(),
  mimeType: z.string(),
  fileSize: z.number().int().positive(),
  pageCount: z.number().int().positive().nullable().optional(),
  embeddings: z.array(EmbeddingChunkSchema),
});

export type DocumentMeta = z.infer<typeof DocumentMetaSchema>;

export const CreateDocumentRequestSchema = DocumentMetaSchema;
export type CreateDocumentRequest = z.infer<typeof CreateDocumentRequestSchema>;

export const CreatedDocumentSchema = z.object({
  id: z.string(),
  name: z.string(),
  mimeType: z.string(),
  fileSize: z.number().int().positive(),
  pageCount: z.number().int().positive().nullable(),
  createdAt: z.string(),
  embeddingCount: z.number().int().nonnegative(),
});

export type CreatedDocument = z.infer<typeof CreatedDocumentSchema>;

export const CreateDocumentResponseSchema = z.object({
  ok: z.literal(true),
  document: CreatedDocumentSchema,
});

export type CreateDocumentResponse = z.infer<
  typeof CreateDocumentResponseSchema
>;
</file>

<file path="packages/llm-client/src/config/ocr-correction-prompt.ts">
export const ocrCorrectionPrompt = `
You are an OCR post-processing engine. You receive OCR output (often Markdown). Produce a corrected version.

Core goal:
- Use your language knowledge and the surrounding context to fix OCR errors that a human would confidently fix.

Rules:
- Output ONLY the corrected text (no preface, no analysis, no commentary).
- Preserve reading order and layout as closely as possible (line breaks, indentation, spacing).
- Preserve Markdown structure where present (headings, lists, checkboxes, code blocks, page separators like "---", "## Page N").
- If the text is from a known source, DO NOT identify it or explain it. Never answer or respond as a chatbot.

What you MAY change (when strongly supported by context):
- Character confusions: O/0, l/1/I, rn/m, S/5, B/8, etc.
- Broken/merged words and spacing around punctuation.
- Hyphenation from line wraps (join words when clearly split).
- Obvious misspellings caused by OCR (use surrounding words to choose the intended spelling).
- Common OCR substitutions in numbers/units/dates (e.g., "l0" -> "10" when clearly numeric).

What you MUST NOT do:
- Do not paraphrase, rewrite for style, summarize, or reorder.
- Do not add information that is not already present.
- Do not "fill in" missing words, names, or numbers unless they are uniquely determined by nearby context.
- If a potential correction is not high-confidence, leave the original text unchanged.

Keep these unchanged:
- Placeholders/tags such as [illegible], [stamp], [signature], [logo], and the "Handwritten:" prefix.

Examples:

Input:
It was the best of timesIt wasthe worst of times.

Output:
It was the best of times
It was the worst of times.
`;
</file>

<file path="packages/llm-client/src/desktop/init/chat.ts">
export const initDesktopLLMClient = async (
  onProgress?: (progress: number) => void,
) => {
  const endpoint = import.meta.env.VITE_DESKTOP_GEN_ENDPOINT;

  if (!endpoint) {
    throw new Error("VITE_DESKTOP_GEN_ENDPOINT is not defined");
  }

  const response = await fetch(`${endpoint}/health`);
  const health = await response.json();
  if (health.status === "ok") {
    onProgress?.(100);
  } else {
    throw new Error("Desktop LLM client is not healthy");
  }
};
</file>

<file path="packages/llm-client/src/desktop/init/embedding.ts">
export const initDesktopEmbeddings = async (
  onProgress?: (progress: number) => void,
) => {
  const endpoint = import.meta.env.VITE_DESKTOP_EMBEDDING_ENDPOINT;

  if (!endpoint) {
    throw new Error("VITE_DESKTOP_EMBEDDING_ENDPOINT is not defined");
  }

  const response = await fetch(`${endpoint}/health`);
  const health = await response.json();
  if (health.status === "ok") {
    onProgress?.(100);
  } else {
    throw new Error("Desktop Embeddings is not healthy");
  }
};
</file>

<file path="packages/llm-client/src/desktop/init/image.ts">
export const initDesktopVisionModel = async (
  onProgress?: (progress: number) => void,
) => {
  const endpoint = import.meta.env.VITE_DESKTOP_VISION_ENDPOINT;

  if (!endpoint) {
    throw new Error("VITE_DESKTOP_VISION_ENDPOINT is not defined");
  }

  const response = await fetch(`${endpoint}/health`);
  const health = await response.json();
  if (health.status === "ok") {
    onProgress?.(100);
  } else {
    throw new Error("Desktop Vision Model is not healthy");
  }
};
</file>

<file path="packages/llm-client/src/desktop/run/chat.ts">
import { createOpenAI } from "@ai-sdk/openai";
import type { Message } from "@hydrowise/entities";
import { streamText } from "ai";

const getOpenAIClient = () =>
  createOpenAI({
    baseURL: import.meta.env.VITE_DESKTOP_GEN_ENDPOINT,
    apiKey: "null",
  });

export const sendDesktopChatCompletion = async (
  messages: Message[],
  onChunk: (chunk: string) => void,
): Promise<string> => {
  const openai = getOpenAIClient();
  const result = streamText({
    model: openai.chat("any"),
    messages: messages.map((m) => ({
      role: m.role,
      content: m.content,
    })),
  });

  const chunks: string[] = [];
  for await (const text of result.textStream) {
    chunks.push(text);
    onChunk(text);
  }

  return chunks.join("");
};
</file>

<file path="packages/llm-client/src/desktop/run/embeddings.ts">
import { createOpenAI } from "@ai-sdk/openai";
import { embedMany } from "ai";

const getOpenAIClient = () =>
  createOpenAI({
    baseURL: import.meta.env.VITE_DESKTOP_EMBEDDING_ENDPOINT,
    apiKey: "null",
  });

export const sendDesktopEmbeddings = async (
  values: string[],
): Promise<number[][]> => {
  const openai = getOpenAIClient();
  const result = await embedMany({
    model: openai.embedding("any"),
    values,
  });

  return result.embeddings;
};
</file>

<file path="packages/llm-client/src/desktop/run/image.ts">
import { createOpenAI } from "@ai-sdk/openai";
import { generateText } from "ai";

const getOpenAIClient = () =>
  createOpenAI({
    baseURL: import.meta.env.VITE_DESKTOP_VISION_ENDPOINT,
    apiKey: "null",
  });

const FileToBase64 = (file: File | Blob): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.readAsDataURL(file);
    reader.onload = () => resolve(reader.result as string);
    reader.onerror = reject;
  });
};

export const processDesktopImage = async (input: File): Promise<string> => {
  const openai = getOpenAIClient();
  const base64 = await FileToBase64(input);
  const result = await generateText({
    model: openai.chat("any"),
    messages: [
      {
        role: "user",
        content: [
          { type: "text", text: "Transcribe the following image" },
          { type: "image", image: base64 },
        ],
      },
    ],
  });

  const text = result.text;
  return text;
};
</file>

<file path="packages/llm-client/src/desktop/run/ocr.ts">
import { createOpenAI } from "@ai-sdk/openai";
import { generateText } from "ai";
import { ocrCorrectionPrompt } from "../../config/ocr-correction-prompt";

const getOpenAIClient = () =>
  createOpenAI({
    baseURL: import.meta.env.VITE_DESKTOP_GEN_ENDPOINT,
    apiKey: "null",
  });

export const postprocessDesktopOcrText = async (
  ocrText: string,
): Promise<string> => {
  const openai = getOpenAIClient();
  const result = await generateText({
    system: ocrCorrectionPrompt,
    model: openai.chat("any"),
    temperature: 0,
    messages: [
      {
        role: "user",
        content: ocrText,
      },
    ],
  });

  return result.text;
};
</file>

<file path="packages/llm-client/src/web/init/chat.ts">
import {
  doesBrowserSupportTransformersJS,
  transformersJS,
} from "@browser-ai/transformers-js";

const modelId = "Xenova/Phi-3-mini-4k-instruct";

const model = transformersJS(modelId, {
  device: "webgpu",
});

export const initWebLLMEngine = async (
  onProgress?: (progress: number) => void,
): Promise<void> => {
  if (!doesBrowserSupportTransformersJS()) {
    return;
  }

  const availability = await model.availability();
  if (availability === "unavailable") {
    return;
  }
  if (availability === "downloadable") {
    await model.createSessionWithProgress(({ progress }) => {
      if (!onProgress || progress === undefined) return;
      onProgress(Math.round(progress * 100));
    });
  }
};

export const getWebLLMEngine = () => {
  return model;
};
</file>

<file path="packages/llm-client/src/web/init/embeddings.ts">
import {
  doesBrowserSupportTransformersJS,
  transformersJS,
} from "@browser-ai/transformers-js";
import { type EmbeddingModel, embedMany } from "ai";

const model = transformersJS.embedding("Xenova/bge-base-en-v1.5", {
  normalize: true,
  pooling: "mean",
});

export const initWebEmbeddings = async (
  onProgress?: (progress: number) => void,
) => {
  if (!doesBrowserSupportTransformersJS()) {
    throw new Error("Browser does not support Transformers.js");
  }

  // ðŸ”¥ FORCE LOAD + COMPILE + CACHE
  await embedMany({
    model,
    values: ["warmup"],
  });

  onProgress?.(100);
};

export const getEmbeddingsModel = (): EmbeddingModel => {
  return model;
};
</file>

<file path="packages/llm-client/src/web/init/image.ts">
import type { Processor, ProgressInfo } from "@huggingface/transformers";
import {
  AutoProcessor,
  Florence2ForConditionalGeneration,
} from "@huggingface/transformers";

const MODEL_ID = "onnx-community/Florence-2-large-ft";

const state: {
  model: Florence2ForConditionalGeneration | null;
  processor: Processor | null;
  initPromise: Promise<void> | null;
} = {
  model: null,
  processor: null,
  initPromise: null,
};

export const initWebVisionModel = async (
  onProgress?: (progress: number) => void,
): Promise<void> => {
  if (state.model && state.processor) {
    onProgress?.(100);
    return;
  }
  if (state.initPromise) return state.initPromise;

  state.initPromise = (async () => {
    try {
      state.model = (await Florence2ForConditionalGeneration.from_pretrained(
        MODEL_ID,
        {
          device: "webgpu",
          dtype: {
            embed_tokens: "fp32",
            vision_encoder: "fp32",
            encoder_model: "q4",
            decoder_model_merged: "q4",
          },
          progress_callback: (data: ProgressInfo) => {
            if (data.status === "progress" && onProgress) {
              onProgress(Math.round(data.progress || 0));
            }
          },
        },
      )) as Florence2ForConditionalGeneration;

      state.processor = await AutoProcessor.from_pretrained(MODEL_ID, {
        progress_callback: (data: ProgressInfo) => {
          if (data.status === "progress" && onProgress) {
            onProgress(Math.round(data.progress || 0));
          }
        },
      });

      onProgress?.(100);
    } catch (error) {
      console.error("Failed to initialize vision model:", error);
      state.initPromise = null;
      throw error;
    }
  })();

  return state.initPromise;
};

export const getWebVisionModel = (): Florence2ForConditionalGeneration => {
  if (!state.model) throw new Error("Vision model still initializing");
  return state.model;
};

export const getWebVisionProcessor = () => {
  if (!state.processor) throw new Error("Vision processor still initializing");
  return state.processor;
};
</file>

<file path="packages/llm-client/src/web/init/logging.ts">
import { env } from "@huggingface/transformers";

// silence third party logs

export const llmLog = {
  error: (message: string, context?: Record<string, unknown>) => {
    console.error(`[llm-client] ${message}`, context ?? "");
  },
  warn: (message: string, context?: Record<string, unknown>) => {
    console.warn(`[llm-client] ${message}`, context ?? "");
  },
  info: (message: string, context?: Record<string, unknown>) => {
    console.info(`[llm-client] ${message}`, context ?? "");
  },
  debug: (message: string, context?: Record<string, unknown>) => {
    console.debug(`[llm-client] ${message}`, context ?? "");
  },
};

let didConfigureWebModelLogging = false;

const shouldSuppressThirdPartyWarning = (args: unknown[]): boolean => {
  return args.some(
    (arg) =>
      typeof arg === "string" &&
      (arg.includes(
        "Unable to determine content-length from response headers",
      ) ||
        arg.includes("VerifyEachNodeIsAssignedToAnEp") ||
        arg.includes("session_state.cc:1280") ||
        arg.includes("session_state.cc:1282")),
  );
};

export const configureWebModelLogging = () => {
  if (didConfigureWebModelLogging) return;
  didConfigureWebModelLogging = true;

  // 1. Configure package-level logging (preferred)
  const onnxBackend = (env as { backends?: Record<string, unknown> }).backends
    ?.onnx as
    | {
        env?: { logLevel?: string; debug?: boolean };
        logLevel?: string;
        debug?: boolean;
        wasm?: { logLevel?: string; debug?: boolean };
      }
    | undefined;

  const targets = [onnxBackend, onnxBackend?.env, onnxBackend?.wasm];
  for (const target of targets) {
    if (!target) continue;
    target.logLevel = "error";
    target.debug = false;
  }

  // 2. Add fallback suppression for persistent/WASM logs
  const originalWarn = console.warn.bind(console);
  const originalError = console.error.bind(console);

  console.warn = (...args: unknown[]) => {
    if (shouldSuppressThirdPartyWarning(args)) return;
    originalWarn(...args);
  };

  console.error = (...args: unknown[]) => {
    if (shouldSuppressThirdPartyWarning(args)) return;
    originalError(...args);
  };
};
</file>

<file path="packages/llm-client/src/web/run/chat.ts">
import type { Message } from "@hydrowise/entities";
import { streamText } from "ai";
import { getWebLLMEngine } from "../init/chat";

export const sendWebChatCompletion = async (
  messages: Message[],
  onChunk: (chunk: string) => void,
): Promise<string> => {
  const result = streamText({
    model: getWebLLMEngine(),
    messages: messages.map((m) => ({
      role: m.role,
      content: m.content,
    })),
  });

  const chunks: string[] = [];
  for await (const text of result.textStream) {
    chunks.push(text);
    onChunk(text);
  }

  return chunks.join("");
};
</file>

<file path="packages/llm-client/src/web/run/embeddings.ts">
import { embedMany } from "ai";
import { getEmbeddingsModel } from "../init/embeddings";

export const sendWebEmbeddings = async (
  values: string[],
): Promise<number[][]> => {
  const model = getEmbeddingsModel();
  const result = await embedMany({
    model,
    values,
  });

  return result.embeddings;
};
</file>

<file path="packages/llm-client/src/web/run/image.ts">
import type { Tensor } from "@huggingface/transformers";
import { RawImage } from "@huggingface/transformers";
import { getWebVisionModel, getWebVisionProcessor } from "../init/image";

export async function processWebImage(input: File): Promise<string> {
  const model = getWebVisionModel();
  const processor = getWebVisionProcessor();

  const image = await RawImage.fromBlob(input);

  const inputs = await processor(image, "<OCR>");

  const sequences = (await model.generate({
    ...inputs,
    max_new_tokens: 1536,
    do_sample: false,
  })) as Tensor;

  const generatedText = processor.batch_decode(sequences, {
    skip_special_tokens: false,
  })[0];

  return generatedText;
}
</file>

<file path="packages/llm-client/src/web/run/ocr.ts">
import { generateText } from "ai";
import { ocrCorrectionPrompt } from "../../config/ocr-correction-prompt";
import { getWebLLMEngine } from "../init/chat";

export const postprocessWebOcrText = async (
  ocrText: string,
): Promise<string> => {
  const result = await generateText({
    model: getWebLLMEngine(),
    temperature: 0,
    messages: [
      {
        role: "system",
        content: ocrCorrectionPrompt,
      },
      {
        role: "user",
        content: ocrText,
      },
    ],
  });

  return result.text;
};
</file>

<file path="packages/llm-client/src/index.ts">
export * from "./client";
</file>

<file path="services/api/src/routes/chat/chat.ts">
// services/api/src/chat/index.ts

import type { DbClient } from "@hydrowise/database";
import { and, chats, eq, messages } from "@hydrowise/database";
import { Hono } from "hono";

const createChatEntity = (userId: string, name?: string) => ({
  id: crypto.randomUUID(),
  userId,
  name: name ?? "New Chat",
});

const getUserId = (c: {
  req: { header: (name: string) => string | undefined };
}) => c.req.header("userId");

const requireUserId = (c: {
  req: { header: (name: string) => string | undefined };
}) => {
  const userId = getUserId(c);
  if (!userId) {
    return { ok: false, error: "userId is required" } as const;
  }
  return { ok: true, userId } as const;
};

export const createChatRoutes = (db: DbClient) => {
  const app = new Hono();

  // GET /chat - list all chats
  app.get("/", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const result = await db
      .select()
      .from(chats)
      .where(eq(chats.userId, user.userId));
    return c.json(result);
  });

  // POST /chat - create a chat
  app.post("/", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const chat = createChatEntity(user.userId);
    await db.insert(chats).values(chat);
    return c.json(chat);
  });

  // GET /chat/:chatId - get single chat
  app.get("/:chatId", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const chatId = c.req.param("chatId");
    const result = await db
      .select()
      .from(chats)
      .where(and(eq(chats.id, chatId), eq(chats.userId, user.userId)));
    return c.json(result[0] ?? null);
  });

  // GET /chat/:chatId/messages - get chat messages
  app.get("/:chatId/messages", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const chatId = c.req.param("chatId");
    const chat = await db
      .select({ id: chats.id })
      .from(chats)
      .where(and(eq(chats.id, chatId), eq(chats.userId, user.userId)));
    if (!chat[0]) {
      return c.json({ ok: false, error: "chat not found" }, 404);
    }
    const result = await db
      .select()
      .from(messages)
      .where(eq(messages.chatId, chatId));
    return c.json(result);
  });

  // DELETE /chat/:chatId - delete a chat
  app.delete("/:chatId", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const chatId = c.req.param("chatId");
    const chat = await db
      .select({ id: chats.id })
      .from(chats)
      .where(and(eq(chats.id, chatId), eq(chats.userId, user.userId)));
    if (!chat[0]) {
      return c.json({ ok: false, error: "chat not found" }, 404);
    }
    await db.delete(messages).where(eq(messages.chatId, chatId));
    const result = await db
      .delete(chats)
      .where(and(eq(chats.id, chatId), eq(chats.userId, user.userId)))
      .returning();
    return c.json(result[0]);
  });

  // POST /chat/:chatId/messages - append message
  app.post("/:chatId/messages", async (c) => {
    const user = requireUserId(c);
    if (!user.ok) return c.json(user, 400);
    const chatId = c.req.param("chatId");
    const chat = await db
      .select({ id: chats.id })
      .from(chats)
      .where(and(eq(chats.id, chatId), eq(chats.userId, user.userId)));
    if (!chat[0]) {
      return c.json({ ok: false, error: "chat not found" }, 404);
    }
    const payload = await c.req.json();
    const role = payload?.role;
    const content = payload?.content;
    if (
      (role !== "user" && role !== "assistant") ||
      typeof content !== "string"
    ) {
      return c.json({ ok: false, error: "role and content are required" }, 400);
    }
    const message = {
      id: crypto.randomUUID(),
      chatId,
      role,
      content,
    };
    await db.insert(messages).values(message);
    return c.json(message);
  });

  return app;
};
</file>

<file path="services/api/src/routes/chat/index.ts">
export { createChatRoutes } from "./chat";
</file>

<file path="services/api/src/routes/document/document.ts">
import type { DbClient } from "@hydrowise/database";
import { documentEmbeddings, documents } from "@hydrowise/database";
import { CreateDocumentRequestSchema } from "@hydrowise/entities";
import { Hono } from "hono";

const getUserId = (c: {
  req: { header: (name: string) => string | undefined };
}) => c.req.header("userId");

const createDocumentEntity = (
  userId: string,
  name: string,
  mimeType: string,
  fileSize: number,
  pageCount: number | null,
) => ({
  id: crypto.randomUUID(),
  userId,
  name,
  mimeType,
  fileSize,
  pageCount,
  createdAt: new Date(),
});

const createDocumentEmbeddingEntity = (
  documentId: string,
  content: string,
  embedding: number[],
  chunkIndex: number,
) => ({
  id: crypto.randomUUID(),
  documentId,
  content,
  embedding,
  chunkIndex,
  createdAt: new Date(),
});

export const createDocumentRoutes = (db: DbClient) => {
  const app = new Hono();

  // POST /document - create a document
  app.post("/", async (c) => {
    const userId = getUserId(c);
    if (!userId) {
      return c.json({ ok: false, error: "userId is required" }, 400);
    }

    // Parse and validate request body using Zod
    const body = await c.req.json();
    const parseResult = CreateDocumentRequestSchema.safeParse(body);

    if (!parseResult.success) {
      return c.json(
        {
          ok: false,
          error: "Invalid request body",
          details: parseResult.error.flatten(),
        },
        400,
      );
    }

    const payload = parseResult.data;

    // Create the document record
    const document = createDocumentEntity(
      userId,
      payload.name,
      payload.mimeType,
      payload.fileSize,
      payload.pageCount ?? null,
    );

    await db.insert(documents).values(document);

    // Create embedding records (if any)
    if (payload.embeddings.length > 0) {
      const embeddingRecords = payload.embeddings.map((chunk, index) =>
        createDocumentEmbeddingEntity(
          document.id,
          chunk.content,
          chunk.embedding,
          index,
        ),
      );

      await db.insert(documentEmbeddings).values(embeddingRecords);
    }

    return c.json({
      ok: true,
      document: {
        id: document.id,
        name: document.name,
        mimeType: document.mimeType,
        fileSize: document.fileSize,
        pageCount: document.pageCount,
        createdAt: document.createdAt,
        embeddingCount: payload.embeddings.length,
      },
    });
  });

  return app;
};
</file>

<file path="services/api/src/routes/document/index.ts">
export * from "./document";
</file>

<file path="services/api/src/routes/rag/index.ts">
export * from "./rag";
</file>

<file path="services/api/src/routes/rag/rag.ts">
import {
  cosineDistance,
  type DbClient,
  desc,
  documentEmbeddings,
  documents,
  eq,
  sql,
} from "@hydrowise/database";
import { Hono } from "hono";

const app = new Hono();

const getUserId = (c: {
  req: { header: (name: string) => string | undefined };
}) => c.req.header("userId");

export const createRagRoutes = (db: DbClient) => {
  app.post("/retrieve-context", async (c) => {
    const userId = getUserId(c);
    if (!userId) {
      return c.json({ ok: false, error: "userId is required" }, 400);
    }
    const body = await c.req.json();
    const userEmbedding = body.embedding;
    if (!userEmbedding) {
      return c.json({ ok: false, error: "embedding is required" }, 400);
    }

    const similarity = sql<number>`1 - (${cosineDistance(
      documentEmbeddings.embedding,
      userEmbedding,
    )})`;

    const results = await db
      .select({
        content: documentEmbeddings.content,
        similarity,
      })
      .from(documentEmbeddings)
      .innerJoin(documents, eq(documentEmbeddings.documentId, documents.id))
      .where(eq(documents.userId, userId))
      .orderBy(desc(similarity))
      .limit(5);

    return c.json({ ok: true, data: results });
  });

  return app;
};
</file>

<file path="services/api/src/config.ts">
export const getConfig = (runtimeEnv: NodeJS.ProcessEnv = process.env) => {
  return {
    mode: (runtimeEnv.RUNTIME_MODE as "web" | "desktop") || "web",
    databaseUrl: runtimeEnv.DATABASE_URL,
    desktopDbPath: runtimeEnv.DESKTOP_DB_PATH || "./hydrowise.db",
  };
};
</file>

<file path="services/api/README.md">
To install dependencies:
```sh
bun install
```

To run:
```sh
bun run dev
```

open http://localhost:3000
</file>

<file path="turbo.json">
{
  "$schema": "https://turbo.build/schema.json",
  "tasks": {
    "build": {
      "dependsOn": ["^build"],
      "outputs": ["dist/**", "!.vite/**"]
    },
    "clean": {
      "cache": false
    },
    "dev": {
      "cache": false,
      "persistent": true
    },
    "dev:web": {
      "cache": false,
      "persistent": true
    },
    "dev:desktop": {
      "cache": false,
      "persistent": true
    },
    "lint": {},
    "format": {},
    "check": {},
    "preview": {
      "cache": false,
      "persistent": true
    }
  }
}
</file>

<file path="apps/web/src/components/UploadDocumentModel.tsx">
import { Box, Button, Dialog, DialogContent, DialogTitle } from "@mui/material";
import { styled } from "@mui/material/styles";
import { CloudUploadIcon } from "lucide-react";
import { useMemo, useState } from "react";
import { useDocument } from "@/hooks/useDocument";

const VisuallyHiddenInput = styled("input")({
  clip: "rect(0 0 0 0)",
  clipPath: "inset(50%)",
  height: 1,
  overflow: "hidden",
  position: "absolute",
  bottom: 0,
  left: 0,
  whiteSpace: "nowrap",
  width: 1,
});

export const UploadDocumentModel = ({
  open,
  onClose,
}: {
  open: boolean;
  onClose: () => void;
}) => {
  const [isDragging, setIsDragging] = useState(false);
  const { uploadDocument } = useDocument();

  const dropzoneStyles = useMemo(
    () => ({
      border: "1px dashed",
      borderColor: isDragging ? "primary.main" : "divider",
      borderRadius: 2,
      px: 3,
      py: 4,
      textAlign: "center",
      backgroundColor: isDragging ? "action.hover" : "background.paper",
      transition: "border-color 160ms ease, background-color 160ms ease",
    }),
    [isDragging],
  );

  const handleFiles = (files: FileList | null) => {
    if (!files || files.length === 0) return;
    uploadDocument(files[0]);
  };

  return (
    <Dialog open={open} onClose={onClose}>
      <DialogTitle>Upload Documents</DialogTitle>
      <DialogContent>
        <Box
          onDragOver={(event) => {
            event.preventDefault();
            setIsDragging(true);
          }}
          onDragLeave={() => setIsDragging(false)}
          onDrop={(event) => {
            event.preventDefault();
            setIsDragging(false);
            handleFiles(event.dataTransfer.files);
          }}
          sx={dropzoneStyles}
        >
          <Box sx={{ display: "flex", flexDirection: "column", gap: 2 }}>
            <Box sx={{ fontSize: 14, color: "text.secondary" }}>
              Drag and drop files here or use the button below
            </Box>
            <Button
              component="label"
              variant="contained"
              tabIndex={-1}
              startIcon={<CloudUploadIcon />}
              sx={{ alignSelf: "center" }}
            >
              Upload files
              <VisuallyHiddenInput
                type="file"
                onChange={(event) => handleFiles(event.target.files)}
                multiple
              />
            </Button>
          </Box>
        </Box>
      </DialogContent>
    </Dialog>
  );
};
</file>

<file path="apps/web/src/hooks/useInitEngine.ts">
import { initAllEngines } from "@hydrowise/llm-client";
import { useEffect, useState } from "react";

type WebLLMEngineState = {
  isLoading: boolean;
  loadProgress: number;
  engineReady: boolean;
};

export const useWebLLMEngine = (): WebLLMEngineState => {
  const [isLoading, setIsLoading] = useState(true);
  const [loadProgress, setLoadProgress] = useState(0);
  const [engineReady, setEngineReady] = useState(false);

  useEffect(() => {
    let isActive = true;

    getOrInitEngine(setLoadProgress)
      .then(() => {
        if (!isActive) return;
        setIsLoading(false);
        setEngineReady(true);
      })
      .catch(() => {
        if (!isActive) return;
        setIsLoading(false);
      });

    return () => {
      isActive = false;
    };
  }, []);

  return {
    isLoading,
    loadProgress,
    engineReady,
  };
};

let engineInitPromise: Promise<void> | null = null;

const getOrInitEngine = (onProgress?: (progress: number) => void) => {
  if (!engineInitPromise) {
    engineInitPromise = initAllEngines(onProgress).then(() => undefined);
  }

  return engineInitPromise;
};
</file>

<file path="packages/core/src/document/docx/parse-docx.ts">
import { gfm } from "@truto/turndown-plugin-gfm";
import { convertToHtml } from "mammoth";
import TurndownService from "turndown";

export const parseDocx = async (file: File) => {
  const { value } = await convertToHtml({
    arrayBuffer: await file.arrayBuffer(),
  });

  const turndown = new TurndownService({
    codeBlockStyle: "fenced",
  });
  turndown.use(gfm);

  const markdown = turndown.turndown(value ?? "");

  return { text: markdown, pageCount: null };
};
</file>

<file path="packages/core/src/document/pdf/parse-pdf.ts">
import { PDFParse } from "pdf-parse";

PDFParse.setWorker(
  "https://cdn.jsdelivr.net/npm/pdf-parse@latest/dist/pdf-parse/web/pdf.worker.mjs",
);

export const parsePdf = async (file: File) => {
  const parser = new PDFParse({ data: await file.arrayBuffer() });
  const result = await parser.getText();
  await parser.destroy();

  const parsed = result as unknown as { text?: string; numpages?: number };
  const pageCount =
    typeof parsed.numpages === "number" ? parsed.numpages : null;

  const text = (parsed.text ?? "").replace(/\f/g, "\n\n---\n\n");

  return {
    text,
    pageCount,
  };
};
</file>

<file path="packages/core/src/utils/chunk.ts">
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";

export type ChunkTextOptions = {
  chunkSize?: number;
  chunkOverlap?: number;
  minChunkSize?: number;
};

export const chunkText = async (
  text: string,
  options: ChunkTextOptions = {},
): Promise<string[]> => {
  const trimmed = text
    .split("\n")
    .filter((line) => !/^--\s*\d+\s+of\s+\d+\s*--$/i.test(line.trim()))
    .join("\n")
    .trim();
  if (!trimmed) return [];

  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: options.chunkSize ?? 1000,
    chunkOverlap: options.chunkOverlap ?? 240,
  });

  const docs = await splitter.createDocuments([trimmed]);
  const chunks = docs.map((doc) => doc.pageContent.trim()).filter(Boolean);
  if (chunks.length <= 1) return chunks;

  const minChunkSize = options.minChunkSize ?? 40;
  const lastIndex = chunks.length - 1;
  if (chunks[lastIndex].length < minChunkSize) {
    chunks[lastIndex - 1] = `${chunks[lastIndex - 1]}\n${chunks[lastIndex]}`;
    chunks.pop();
  }

  return chunks;
};
</file>

<file path="packages/core/tsconfig.json">
{
  "compilerOptions": {
    "declaration": true,
    "declarationMap": true,
    "emitDeclarationOnly": false,
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "outDir": "dist",
    "rootDir": "src",
    "target": "ES2022",
    "skipLibCheck": true
  },
  "include": ["src/**/*"]
}
</file>

<file path="packages/db/src/index.ts">
import type { NodePgDatabase } from "drizzle-orm/node-postgres";
import type { PgliteDatabase } from "drizzle-orm/pglite";
import type * as schema from "./schema";

export {
  and,
  asc,
  cosineDistance,
  desc,
  eq,
  gt,
  gte,
  hammingDistance,
  ilike,
  inArray,
  innerProduct,
  jaccardDistance,
  l1Distance,
  l2Distance,
  lt,
  lte,
  ne,
  or,
  sql,
} from "drizzle-orm";
export { createDesktopClient } from "./desktop";
export * from "./schema";
export { createWebClient } from "./web";
export type WebDbClient = NodePgDatabase<typeof schema>;
export type LocalDbClient = PgliteDatabase<typeof schema>;
export type DbClient = WebDbClient | LocalDbClient;
</file>

<file path="packages/db/src/schema.ts">
import {
  index,
  integer,
  pgEnum,
  pgTable,
  text,
  timestamp,
  vector,
} from "drizzle-orm/pg-core";

export const users = pgTable("users", {
  id: text("id").primaryKey(),
  email: text("email").notNull().unique(),
  createdAt: timestamp("created_at").defaultNow().notNull(),
});

export const messageRole = pgEnum("message_role", ["user", "assistant"]);

export const chats = pgTable(
  "chats",
  {
    id: text("id").primaryKey(),
    userId: text("user_id")
      .notNull()
      .references(() => users.id),
    name: text("name").notNull(),
    createdAt: timestamp("created_at").defaultNow().notNull(),
  },
  (table) => [index("chats_user_id_idx").on(table.userId)],
);

export const messages = pgTable(
  "messages",
  {
    id: text("id").primaryKey(),
    chatId: text("chat_id").notNull(),
    role: messageRole("role").notNull(),
    content: text("content").notNull(),
    createdAt: timestamp("created_at").defaultNow().notNull(),
  },
  (table) => [index("messages_chat_id_idx").on(table.chatId)],
);

export const documents = pgTable(
  "documents",
  {
    id: text("id").primaryKey(),
    userId: text("user_id")
      .notNull()
      .references(() => users.id),
    name: text("name").notNull(),
    mimeType: text("mime_type").notNull(),
    fileSize: integer("file_size").notNull(),
    pageCount: integer("page_count"),
    createdAt: timestamp("created_at").defaultNow().notNull(),
  },
  (table) => [index("documents_user_id_idx").on(table.userId)],
);

export const documentEmbeddings = pgTable(
  "document_embeddings",
  {
    id: text("id").primaryKey(),
    documentId: text("document_id")
      .notNull()
      .references(() => documents.id),
    content: text("content").notNull(),
    embedding: vector("embedding", { dimensions: 768 }).notNull(),
    chunkIndex: integer("chunk_index").notNull(),
    createdAt: timestamp("created_at").defaultNow().notNull(),
  },
  (table) => [
    index("document_embeddings_document_id_idx").on(table.documentId),
  ],
);
</file>

<file path="packages/db/tsconfig.json">
{
  "compilerOptions": {
    "declaration": true,
    "declarationMap": true,
    "emitDeclarationOnly": false,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "outDir": "dist",
    "rootDir": "src",
    "target": "ES2023",
    "strict": true,
    "skipLibCheck": true,
    "esModuleInterop": true
  },
  "include": ["src/**/*"]
}
</file>

<file path="packages/entities/src/Message.ts">
import { z } from "zod";

export const MessageSchema = z.object({
  role: z.enum(["user", "assistant"]),
  content: z.string(),
});

export type Message = z.infer<typeof MessageSchema>;
</file>

<file path="packages/entities/tsconfig.json">
{
  "compilerOptions": {
    "declaration": true,
    "declarationMap": true,
    "emitDeclarationOnly": false,
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "outDir": "dist",
    "rootDir": "src",
    "target": "ES2022",
    "skipLibCheck": true
  },
  "include": ["src/**/*"]
}
</file>

<file path="packages/llm-client/src/config.ts">
export const LLM_CONFIG = {
  model: "Llama-3.2-1B-Instruct-q4f16_1-MLC",
  temperature: 0.6,
  maxTokens: 128,
} as const;

export const EMBEDDING_CONFIG = {
  model: "Xenova/bge-base-en-v1.5",
  normalize: true,
  pooling: "mean",
} as const;
</file>

<file path="packages/llm-client/tsconfig.json">
{
  "compilerOptions": {
    "skipLibCheck": true,
    "declaration": true,
    "declarationMap": true,
    "emitDeclarationOnly": false,
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "outDir": "dist",
    "rootDir": "src",
    "target": "ES2022",
    "types": ["vite/client"]
  },
  "include": ["src/**/*"]
}
</file>

<file path="packages/llm-client/vite.config.ts">
import { resolve } from "node:path";
import { fileURLToPath } from "node:url";
import { defineConfig } from "vite";
import dts from "vite-plugin-dts";

export default defineConfig({
  plugins: [dts({ rollupTypes: true })],
  resolve: {
    alias: {
      "@": fileURLToPath(new URL("./src", import.meta.url)),
    },
  },
  build: {
    lib: {
      entry: resolve(__dirname, "src/index.ts"),
      name: "HydrowiseLlmClient",
      fileName: "index",
      formats: ["es", "cjs"],
    },
    rollupOptions: {
      external: ["@mlc-ai/web-llm"],
      output: {
        globals: {
          "@mlc-ai/web-llm": "WebLLM",
        },
      },
    },
  },
});
</file>

<file path="services/api/package.json">
{
  "name": "@hydrowise/api",
  "scripts": {
    "dev": "bun run --hot src/index.ts",
    "dev:web": "bun run --hot src/index.ts",
    "dev:desktop": "RUNTIME_MODE=desktop bun run --hot src/index.ts",
    "clean": "rm -rf dist",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check ."
  },
  "dependencies": {
    "@hydrowise/entities": "workspace:*",
    "@hydrowise/database": "workspace:*",
    "hono": "^4.11.7"
  },
  "devDependencies": {
    "@types/bun": "latest"
  }
}
</file>

<file path="services/api/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "moduleResolution": "bundler",
    "verbatimModuleSyntax": true,
    "strict": true,
    "jsx": "react-jsx",
    "jsxImportSource": "hono/jsx",
    "paths": {
      "@hydrowise/entities": ["../../packages/entities/src/index.ts"],
      "@hydrowise/database": ["../../packages/db/src/index.ts"]
    }
  }
}
</file>

<file path="biome.json">
{
  "$schema": "https://biomejs.dev/schemas/2.3.13/schema.json",
  "formatter": {
    "indentStyle": "space",
    "indentWidth": 2
  },
  "linter": {
    "rules": {
      "recommended": true
    }
  },
  "assist": {
    "actions": {
      "source": {
        "organizeImports": "on"
      }
    }
  },
  "css": {
    "parser": {
      "cssModules": true,
      "tailwindDirectives": true
    }
  }
}
</file>

<file path="apps/web/src/components/HydroSidebar.tsx">
import type { Chat } from "@hydrowise/entities";
import { Box, Button, Stack } from "@mui/material";
import { Trash } from "lucide-react";
import { useChat } from "@/hooks/useChat";
import { useChatStore } from "@/store/chatStore";

export const HydroSidebar = () => {
  const { chats, createChatMutation, deleteChatMutation } = useChat();
  const { setActiveChatId } = useChatStore();
  return (
    <Box
      sx={{
        width: 256,
        height: "100vh",
        flexShrink: 0,
        borderRight: "1px solid",
        borderColor: "divider",
      }}
    >
      <Stack>
        <Button onClick={() => createChatMutation.mutate()}>New Chat</Button>
        {chats?.map((chat: Chat) => (
          <Box key={chat.id}>
            <button type="button" onClick={() => setActiveChatId(chat.id)}>
              {chat.name}
            </button>
            <Button
              startIcon={<Trash />}
              onClick={() => deleteChatMutation.mutate(chat.id)}
            ></Button>
          </Box>
        ))}
      </Stack>
    </Box>
  );
};
</file>

<file path="apps/web/src/App.tsx">
import { Box } from "@mui/material";
import { HydroChat } from "@/components/HydroChat";
import { HydroSidebar } from "@/components/HydroSidebar";

function App() {
  return (
    <Box
      sx={{
        display: "flex",
        height: "100vh",
        overflow: "hidden",
        bgcolor: "background.default",
      }}
    >
      <HydroSidebar />
      <Box
        sx={{
          flex: 1,
          minWidth: 0,
          height: "100%",
          overflow: "hidden",
        }}
      >
        <HydroChat />
      </Box>
    </Box>
  );
}

export default App;
</file>

<file path="apps/web/src/main.tsx">
import { CssBaseline, ThemeProvider } from "@mui/material";
import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
import { StrictMode } from "react";
import { createRoot } from "react-dom/client";
import theme from "@/theme";
import App from "./App";

const root = document.getElementById("root");
if (!root) throw new Error("Root not found");

const queryClient = new QueryClient();

createRoot(root).render(
  <StrictMode>
    <QueryClientProvider client={queryClient}>
      <ThemeProvider theme={theme}>
        <CssBaseline />
        <App />
      </ThemeProvider>
    </QueryClientProvider>
  </StrictMode>,
);
</file>

<file path="apps/web/tsconfig.app.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "types": ["vite/client"],
    "skipLibCheck": true,

    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",
    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"],
      "@src/*": ["./src/*"],
      "@hydrowise/core": ["../../packages/core/src/index.ts"],
      "@hydrowise/entities": ["../../packages/entities/src/index.ts"],
      "@hydrowise/llm-client": ["../../packages/llm-client/src/index.ts"]
    },

    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}
</file>

<file path="packages/core/src/document/parse.ts">
import type { DocumentMeta } from "@hydrowise/entities";
import { chunkText } from "../utils/chunk";
import { parseDocx } from "./docx/parse-docx";
import { generateEmbeddings } from "./embeddings";
import { parseImage } from "./image/parse-image";
import { parsePdf } from "./pdf/parse-pdf";
import { parsePptx } from "./pptx/parse-pptx";

type ParsedDocument = {
  text: string;
  pageCount?: number | null;
};

type Parser = (file: File) => Promise<ParsedDocument>;

const IMAGE_FILE_EXTENSIONS = new Set([
  "jpg",
  "jpeg",
  "png",
  "webp",
  "heic",
  "heif",
  "gif",
  "bmp",
  "tif",
  "tiff",
  "avif",
]);

const parsersByMimeType: Record<string, Parser> = {
  "application/pdf": parsePdf,
  "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
    parseDocx,
  "application/vnd.openxmlformats-officedocument.presentationml.presentation":
    parsePptx,
};

const hasKnownImageExtension = (filename: string) => {
  const extension = filename.split(".").pop()?.toLowerCase();
  return !!extension && IMAGE_FILE_EXTENSIONS.has(extension);
};

const getParserForFile = (file: File): Parser | undefined => {
  const parser = parsersByMimeType[file.type];
  if (parser) return parser;

  if (file.type.toLowerCase().startsWith("image/")) {
    return parseImage;
  }

  if (!file.type && hasKnownImageExtension(file.name)) {
    return parseImage;
  }

  return undefined;
};

export const parseDocumentMeta = async (
  file: File,
  options: {
    onProgress?: (completed: number, total: number) => void;
    chunkOptions?: {
      chunkSize?: number;
      chunkOverlap?: number;
    };
  } = {},
): Promise<DocumentMeta> => {
  const parser = getParserForFile(file);
  if (!parser) {
    throw new Error(
      `Unsupported file type: ${file.type}. ` + `File: ${file.name}`,
    );
  }

  const parsedDocument = await parser(file);
  const chunks = await chunkText(parsedDocument.text, options.chunkOptions);
  const embeddings = await generateEmbeddings(chunks, options.onProgress);

  return {
    name: file.name,
    mimeType: file.type,
    fileSize: file.size,
    pageCount: parsedDocument.pageCount ?? null,
    embeddings,
  };
};
</file>

<file path="packages/db/package.json">
{
  "name": "@hydrowise/database",
  "version": "0.0.0",
  "private": true,
  "type": "module",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "clean": "rm -rf dist tsconfig.tsbuildinfo",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check .",
    "drizzle:generate": "drizzle-kit generate",
    "drizzle:migrate": "drizzle-kit migrate",
    "drizzle:push": "drizzle-kit push",
    "drizzle:studio": "drizzle-kit studio"
  },
  "devDependencies": {
    "@types/bun": "latest",
    "@types/pg": "^8.16.0",
    "drizzle-kit": "^0.31.8"
  },
  "peerDependencies": {
    "typescript": "^5"
  },
  "dependencies": {
    "@electric-sql/pglite": "^0.3.15",
    "dotenv": "^17.2.3",
    "drizzle-orm": "^0.45.1",
    "pg": "^8.18.0"
  },
  "exports": {
    ".": {
      "import": "./dist/index.js",
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    }
  }
}
</file>

<file path="packages/entities/src/index.ts">
export * from "./Chat";
export * from "./Document";
export * from "./Message";
</file>

<file path="packages/entities/package.json">
{
  "name": "@hydrowise/entities",
  "version": "0.0.0",
  "private": true,
  "type": "module",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "dependencies": {
    "zod": "^3.22.4"
  },
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "clean": "rm -rf dist tsconfig.tsbuildinfo",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check ."
  },
  "exports": {
    ".": {
      "import": "./dist/index.js",
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    }
  }
}
</file>

<file path=".gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

**/**/.env

/tools

# Monorepo / Tooling
.turbo
*.tsbuildinfo
coverage
.cache

# Migrations
packages/db/drizzle

# Rust / Tauri
src-tauri/target
target

# Database
*.db
*.sqlite
hydrowise.db/
</file>

<file path="apps/web/src/components/ui/HydroInput.tsx">
import { TextField } from "@mui/material";
import type { ChangeEvent, KeyboardEvent } from "react";

type Props = {
  value: string;
  onInput: (message: string) => void;
  onSend: (message: string) => void;
  disabled?: boolean;
};

export const HydroInput = ({
  value,
  onInput,
  onSend,
  disabled = false,
}: Props) => {
  const handleChange = (event: ChangeEvent<HTMLInputElement>) => {
    onInput(event.target.value);
  };

  const handleKeyDown = (event: KeyboardEvent<HTMLInputElement>) => {
    if (event.key !== "Enter" || event.shiftKey) return;
    event.preventDefault();
    onSend(value);
  };

  return (
    <TextField
      fullWidth
      placeholder="Ask anything about your notes, courses, or docs"
      value={value}
      onChange={handleChange}
      onKeyDown={handleKeyDown}
      disabled={disabled}
      size="small"
    />
  );
};
</file>

<file path="apps/web/src/store/chatStore.ts">
import { create } from "zustand";

interface ChatStore {
  activeChatId: string | null;
  setActiveChatId: (id: string | null) => void;
  streamingContent: string | null;
  isStreaming: boolean;
  setStreamingContent: (content: string | null) => void;
  appendStreamingChunk: (chunk: string) => void;
  clearStreaming: () => void;
}

export const useChatStore = create<ChatStore>((set) => ({
  activeChatId: null,
  setActiveChatId: (id) => set({ activeChatId: id }),
  streamingContent: null,
  isStreaming: false,
  setStreamingContent: (content) =>
    set({ streamingContent: content, isStreaming: content !== null }),
  appendStreamingChunk: (chunk) =>
    set((state) => ({
      streamingContent: `${state.streamingContent ?? ""}${chunk}`,
      isStreaming: true,
    })),
  clearStreaming: () => set({ streamingContent: null, isStreaming: false }),
}));
</file>

<file path="packages/core/package.json">
{
  "name": "@hydrowise/core",
  "version": "0.0.0",
  "private": true,
  "type": "module",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "clean": "rm -rf dist tsconfig.tsbuildinfo",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check ."
  },
  "exports": {
    ".": {
      "import": "./dist/index.js",
      "types": "./dist/index.d.ts",
      "default": "./dist/index.js"
    }
  },
  "dependencies": {
    "@hydrowise/entities": "workspace:*",
    "@hydrowise/llm-client": "workspace:*",
    "@jvmr/pptx-to-html": "^1.0.0",
    "@truto/turndown-plugin-gfm": "^1.0.2",
    "heic-to": "^1.4.2",
    "langchain": "^0.3.33",
    "mammoth": "^1.11.0",
    "pdf-parse": "^2.4.5",
    "turndown": "^7.2.2"
  },
  "devDependencies": {
    "@types/turndown": "^5.0.6"
  }
}
</file>

<file path="apps/web/src/components/ui/HydroButton.tsx">
import { Button } from "@mui/material";

type Props = {
  onClick: () => void;
  disabled?: boolean;
  label?: string;
  icon?: React.ReactNode;
};

export const HydroButton = ({
  onClick,
  disabled = false,
  label,
  icon,
}: Props) => {
  return (
    <Button
      variant="contained"
      startIcon={icon}
      disabled={disabled}
      onClick={onClick}
    >
      {label}
    </Button>
  );
};
</file>

<file path="apps/web/src/components/ui/HydroMessage.tsx">
import type { Message } from "@hydrowise/entities";
import { Paper, Typography } from "@mui/material";

type Props = {
  message: Message;
};

export const HydroMessage = ({ message }: Props) => {
  return (
    <Paper elevation={1}>
      <Typography variant="body2">{message.content}</Typography>
    </Paper>
  );
};
</file>

<file path="services/api/src/index.ts">
import { createWebClient, type DbClient } from "@hydrowise/database";
import { Hono } from "hono";
import { cors } from "hono/cors";
import { getConfig } from "./config";
import { createChatRoutes } from "./routes/chat";
import { createDocumentRoutes } from "./routes/document";
import { createRagRoutes } from "./routes/rag";

const app = new Hono();

const config = getConfig();

if (!config.databaseUrl) {
  throw new Error("DATABASE_URL is required");
}

console.log(`Starting in ${config.mode.toUpperCase()} mode`);
// Always use Web Client (Postgres) for now, even in desktop mode
const db: DbClient = createWebClient(config.databaseUrl);

app.use(
  "*",
  cors({
    origin: "*",
    allowMethods: ["GET", "POST", "DELETE", "OPTIONS"],
    allowHeaders: ["Content-Type", "userId"],
  }),
);

app.get("/", (c) => {
  return c.text("Hello Hono!");
});

app.route("/chat", createChatRoutes(db));
app.route("/document", createDocumentRoutes(db));
app.route("/rag", createRagRoutes(db));

export default app;
</file>

<file path="package.json">
{
  "name": "hydrowise",
  "private": true,
  "packageManager": "bun@1.3.8",
  "workspaces": [
    "apps/*",
    "services/*",
    "packages/*"
  ],
  "scripts": {
    "dev": "turbo dev",
    "dev:web": "turbo dev:web",
    "dev:desktop": "turbo dev:desktop",
    "build": "turbo build",
    "build:packages": "turbo build --filter=./packages/*",
    "build:apps": "turbo build --filter=./apps/*",
    "clean": "turbo clean",
    "clean:build": "turbo clean && turbo build",
    "nuke": "turbo clean && rm -rf node_modules **/node_modules",
    "preview": "turbo preview --filter=@hydrowise/web",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check . --write"
  },
  "devDependencies": {
    "@biomejs/biome": "^2.3.13",
    "turbo": "^2.8.1",
    "typescript": "~5.9.3"
  }
}
</file>

<file path="apps/web/src/components/ui/HydroInputContainer.tsx">
import { LinearProgress, Stack, Typography } from "@mui/material";
import { Send, Upload } from "lucide-react";
import { useState } from "react";
import { UploadDocumentModel } from "@/components/UploadDocumentModel";
import { HydroButton } from "@/components/ui/HydroButton";
import { HydroInput } from "@/components/ui/HydroInput";

type Props = {
  onSend: (message: string) => void;
  disabled?: boolean;
  loadingProgress?: number;
};

export const HydroInputContainer = ({
  onSend,
  disabled = false,
  loadingProgress = 0,
}: Props) => {
  const [message, setMessage] = useState("");
  const [open, setOpen] = useState(false);

  const handleSend = (content: string) => {
    setMessage("");
    onSend(content);
  };

  return (
    <>
      <Stack spacing={1.5}>
        {loadingProgress < 100 && (
          <Stack direction="row" spacing={2} alignItems="center">
            <LinearProgress variant="determinate" value={loadingProgress} />
            <Typography variant="caption" color="text.secondary">
              Loading model {loadingProgress}%
            </Typography>
          </Stack>
        )}

        <Stack direction={{ xs: "column", sm: "row" }} spacing={1.5}>
          <HydroButton
            onClick={() => setOpen(true)}
            disabled={disabled}
            label="Upload"
            icon={<Upload size={16} />}
          />
          <HydroInput
            value={message}
            onInput={setMessage}
            onSend={handleSend}
            disabled={disabled}
          />
          <HydroButton
            onClick={() => handleSend(message)}
            disabled={disabled || !message}
            label="Send"
            icon={<Send size={16} />}
          />
        </Stack>
      </Stack>

      <UploadDocumentModel open={open} onClose={() => setOpen(false)} />
    </>
  );
};
</file>

<file path="apps/web/package.json">
{
  "name": "@hydrowise/web",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "dev:web": "vite",
    "dev:desktop": "VITE_RUNTIME=desktop vite",
    "build": "tsc -b && vite build",
    "preview": "vite preview",
    "clean": "rm -rf dist",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check ."
  },
  "dependencies": {
    "@hydrowise/entities": "workspace:*",
    "@hydrowise/llm-client": "workspace:*",
    "@hydrowise/core": "workspace:*",
    "@mui/material": "^7.3.4",
    "@emotion/react": "^11.14.0",
    "@emotion/styled": "^11.14.0",
    "lucide-react": "^0.563.0",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "zustand": "^5.0.10"
  },
  "devDependencies": {
    "@tanstack/react-query": "^5.90.20",
    "@types/node": "^24.10.1",
    "@types/react": "^19.2.5",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.1",
    "typescript": "~5.9.3",
    "vite": "^7.2.4"
  }
}
</file>

<file path="apps/web/vite.config.ts">
import { fileURLToPath } from "node:url";
import react from "@vitejs/plugin-react";
import { defineConfig } from "vite";

export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      "@": fileURLToPath(new URL("./src", import.meta.url)),
      "@hydrowise/core": fileURLToPath(
        new URL("../../packages/core/src/index.ts", import.meta.url),
      ),
      "@hydrowise/entities": fileURLToPath(
        new URL("../../packages/entities/src/index.ts", import.meta.url),
      ),
      "@hydrowise/llm-client": fileURLToPath(
        new URL("../../packages/llm-client/src/index.ts", import.meta.url),
      ),
    },
  },
  server: {
    headers: {
      "Cross-Origin-Opener-Policy": "same-origin",
      "Cross-Origin-Embedder-Policy": "require-corp",
    },
  },
  preview: {
    headers: {
      "Cross-Origin-Opener-Policy": "same-origin",
      "Cross-Origin-Embedder-Policy": "require-corp",
    },
  },
});
</file>

<file path="packages/llm-client/package.json">
{
  "name": "@hydrowise/llm-client",
  "version": "0.0.0",
  "private": true,
  "type": "module",
  "main": "./dist/index.js",
  "module": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "scripts": {
    "build": "vite build",
    "clean": "rm -rf dist",
    "lint": "biome lint .",
    "format": "biome format . --write",
    "check": "biome check .",

    "llm": "../../tools/llama-b7876/llama-server -m ../../tools/models/qwen2.5-7b-instruct-q4_k_m.gguf --port 8080 --ctx-size 4096 -ngl 99 --parallel 2",
    "embeddings": "../../tools/llama-b7876/llama-server -m ../../tools/models/nomic-embed-text-v1.5.Q4_0.gguf --port 8081 --embeddings --pooling mean -ngl 99",
    "ocr": "../../tools/llama-b7876/llama-server -m ../../tools/models/LightOnOCR-2-1B-Q4_K_M.gguf --mmproj ../../tools/models/LightOnOCR-2-1B-mmproj-f16.gguf --port 8082 --ctx-size 4096 -ngl 99",
    "dev:desktop": "bun run llm & bun run embeddings & bun run ocr & wait",
    "dev:desktop:all": "bun run llm & bun run embeddings & bun run ocr & wait"
  },
  "dependencies": {
    "@ai-sdk/openai": "^3.0.24",
    "@browser-ai/transformers-js": "^2.0.2",
    "@browser-ai/web-llm": "^2.0.3",
    "@huggingface/transformers": "^3.8.1",
    "@mlc-ai/web-llm": "^0.2.79",
    "ai": "^6.0.66"
  },
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "import": "./dist/index.js",
      "require": "./dist/index.cjs"
    }
  },
  "devDependencies": {
    "vite": "^7.2.4",
    "vite-plugin-dts": "^4.5.4"
  }
}
</file>

<file path="apps/web/src/hooks/useChat.ts">
import { sendChatCompletion } from "@hydrowise/llm-client";
import { useMutation, useQuery, useQueryClient } from "@tanstack/react-query";
import {
  appendMessage,
  createChat,
  deleteChat,
  getChats,
  getMessages,
} from "@/api/chat";
import { getRagApi } from "@/api/rag";
import { useChatStore } from "@/store/chatStore";

export const useChat = () => {
  const queryClient = useQueryClient();
  const {
    activeChatId,
    setActiveChatId,
    streamingContent,
    isStreaming,
    setStreamingContent,
    appendStreamingChunk,
    clearStreaming,
  } = useChatStore();

  const {
    data: chats,
    isLoading: chatsLoading,
    error: chatsError,
  } = useQuery({
    queryKey: ["chats"],
    queryFn: () => getChats(),
  });

  const createChatMutation = useMutation({
    mutationFn: async () => createChat(),
    onSuccess: (chat) => {
      if (chat?.id) {
        setActiveChatId(chat.id);
      }
      queryClient.invalidateQueries({ queryKey: ["chats"] });
    },
  });

  const deleteChatMutation = useMutation({
    mutationFn: (chatId: string) => deleteChat(chatId),
    onSuccess: (_deletedChat, chatId) => {
      if (activeChatId === chatId) {
        setActiveChatId(null);
      }
      queryClient.invalidateQueries({ queryKey: ["chats"] });
    },
  });

  const {
    data: messages,
    isLoading: messagesLoading,
    error: messagesError,
  } = useQuery({
    queryKey: ["messages", activeChatId],
    queryFn: () => getMessages(activeChatId ?? ""),
    enabled: !!activeChatId,
    staleTime: 1000 * 60 * 5,
  });

  const appendMessageMutation = useMutation({
    mutationFn: ({
      chatId,
      role,
      content,
    }: {
      chatId: string;
      role: "user" | "assistant";
      content: string;
    }) => appendMessage(chatId, role, content),
    onMutate: async ({ chatId, role, content }) => {
      const queryKey = ["messages", chatId] as const;
      await queryClient.cancelQueries({ queryKey });
      const previous = queryClient.getQueryData(queryKey);
      queryClient.setQueryData(queryKey, (old: typeof messages | undefined) => [
        ...(old ?? []),
        { role, content },
      ]);
      return { previous, queryKey };
    },
    onError: (_error, _variables, context) => {
      if (context?.previous && context.queryKey) {
        queryClient.setQueryData(context.queryKey, context.previous);
      }
    },
    onSuccess: (_message, { chatId }) => {
      queryClient.invalidateQueries({ queryKey: ["messages", chatId] });
    },
  });

  const submitMessage = async (prompt: string) => {
    const chatId =
      activeChatId ?? (await createChatMutation.mutateAsync()).id ?? null;
    if (!activeChatId && chatId) {
      setActiveChatId(chatId);
    }
    if (!chatId) return;

    const history = messages ?? [];

    await appendMessageMutation.mutateAsync({
      chatId,
      role: "user",
      content: prompt,
    });

    setStreamingContent("");

    const ragContext = await getRagApi(prompt);

    try {
      const response = await sendChatCompletion(
        [
          ...history,
          {
            role: "user",
            content: `Context: ${ragContext} \n\n Prompt: ${prompt}`,
          },
        ],
        (chunk) => appendStreamingChunk(chunk),
      );

      clearStreaming();
      await appendMessageMutation.mutateAsync({
        chatId,
        role: "assistant",
        content: response,
      });
    } catch (error) {
      clearStreaming();
      throw error;
    }
  };

  return {
    submitMessage,
    chats: chats ?? [],
    chatsLoading,
    chatsError,
    messages,
    messagesLoading,
    messagesError,
    streamingContent,
    isStreaming,
    deleteChatMutation,
    createChatMutation,
    appendMessageMutation,
  };
};
</file>

<file path="packages/llm-client/src/client.ts">
import type { Message } from "@hydrowise/entities";
import { initDesktopLLMClient } from "./desktop/init/chat";
import { initDesktopEmbeddings } from "./desktop/init/embedding";
import { initDesktopVisionModel } from "./desktop/init/image";
import { sendDesktopChatCompletion } from "./desktop/run/chat";
import { sendDesktopEmbeddings } from "./desktop/run/embeddings";
import { processDesktopImage } from "./desktop/run/image";
import { postprocessDesktopOcrText } from "./desktop/run/ocr";
import { initWebLLMEngine } from "./web/init/chat";
import { initWebEmbeddings } from "./web/init/embeddings";
import { initWebVisionModel } from "./web/init/image";
import { configureWebModelLogging } from "./web/init/logging";
import { sendWebChatCompletion } from "./web/run/chat";
import { sendWebEmbeddings } from "./web/run/embeddings";
import { processWebImage } from "./web/run/image";
import { postprocessWebOcrText } from "./web/run/ocr";

const getRuntimeMode = () => {
  // Handle Vite/Browser environment
  if (typeof import.meta !== "undefined" && import.meta.env?.VITE_RUNTIME) {
    return import.meta.env.VITE_RUNTIME;
  }
  // Handle Node/Bun environment
  if (typeof process !== "undefined" && process.env?.RUNTIME_MODE) {
    return process.env.RUNTIME_MODE;
  }
  return "web";
};

export const sendChatCompletion = (
  messages: Message[],
  onChunk: (chunk: string) => void,
) => {
  const mode = getRuntimeMode();
  return mode === "web"
    ? sendWebChatCompletion(messages, onChunk)
    : sendDesktopChatCompletion(messages, onChunk);
};

export const sendEmbeddings = async (values: string[]) => {
  const mode = getRuntimeMode();
  return mode === "web"
    ? await sendWebEmbeddings(values)
    : await sendDesktopEmbeddings(values);
};

export const processImage = async (image: File) => {
  const mode = getRuntimeMode();
  return mode === "web" ? processWebImage(image) : processDesktopImage(image);
};

export const postprocessOcrText = async (ocrText: string) => {
  const mode = getRuntimeMode();
  return mode === "web"
    ? postprocessWebOcrText(ocrText)
    : postprocessDesktopOcrText(ocrText);
};

export const initLLMClient = (onProgress?: (progress: number) => void) => {
  const mode = getRuntimeMode();
  if (mode === "web") configureWebModelLogging();
  return mode === "web"
    ? initWebLLMEngine(onProgress)
    : initDesktopLLMClient(onProgress);
};

export const initVisionModel = (onProgress?: (progress: number) => void) => {
  const mode = getRuntimeMode();
  if (mode === "web") configureWebModelLogging();
  return mode === "web"
    ? initWebVisionModel(onProgress)
    : initDesktopVisionModel(onProgress);
};

export const initEmbeddings = (onProgress?: (progress: number) => void) => {
  const mode = getRuntimeMode();
  if (mode === "web") configureWebModelLogging();
  return mode === "web"
    ? initWebEmbeddings(onProgress)
    : initDesktopEmbeddings(onProgress);
};

export const initAllEngines = (onProgress?: (progress: number) => void) => {
  const mode = getRuntimeMode();
  if (mode === "web") configureWebModelLogging();
  return mode === "web"
    ? Promise.all([
        initWebLLMEngine(onProgress),
        initWebVisionModel(onProgress),
        initWebEmbeddings(onProgress),
      ])
    : Promise.all([
        initDesktopLLMClient(onProgress),
        initDesktopVisionModel(onProgress),
        initDesktopEmbeddings(onProgress),
      ]);
};
</file>

<file path="Readme.md">
# Stack

### Frontend

- **Framework**: React (Vite) + TypeScript
- **Auth**: AuthJS (web) + cached session/token (desktop/offline)
- **Styling**: MUI
- **Linting / Formatting**: Biome

---

### Backend

- **Runtime**: Node.js
- **API**: Hono
  - Single codebase
  - Runs in **web mode** or **desktop mode**

---

### Database (source of truth)

- **Web**: PostgreSQL
- **Desktop**: PGlite (embedded Postgres, local file)
- **ORM**: Drizzle
  - Relational + app data
  - Embeddings stored via pgvector

---

### Vector Store (derived data)

- **Embeddings (web + desktop)**: **pgvector** (Postgres extension)
  - Desktop: PGlite + pgvector extension
  - Web: Postgres + pgvector extension

---

### Inference

- **Web**: WebLLM (in-browser via @browser-ai/web-llm, Vercel AI SDK)
- **Desktop**: llama.cpp (local runner / sidecar via @ai-sdk/openai)

# Features:

all of these features should allow for "@"ing courses/documents/terms???

- Standard chat with memory
- Flashcard generation
- Coursework quiz generation
- Notes assistant
- Auto Analysis "Topic" of input data
  - detect could be on test topics based on size/focus on a topic within a document

### Gamification

- track the amount of wattage the computer is using and convert it to L/consumed
- compare to real world water usage ie. showers, baths, etc.

### Performance Controls

- Allow users to dial how much performance of there computer they want to use (CPU/GPU)
- Find wattage Draw for the given thing they are doing. maybe a cool analytics page


# Business Logic

- Open Source
- Point users to download the desktop app for more performance + full privacy
- Explain Deeply what is going on when you run a LLM on your machine. Animation?

# Models to use

## Web (WebLLM)

LLM

- Llama 3.1 8B Instruct (Q4_K_M)
- Qwen2.5 7B Instruct (Q4_K_M)

Embeddings

- nomic-embed-text-v1.5 (Q4_0)
- bge-small-en-v1.5 (Q4_0)

## Local (llama.cpp, Metal)

Mac M1

- LLM: Llama 3.1 8B Instruct (Q4_K_M) or Mistral 7B Instruct (Q4_K_M)
- Embeddings: nomic-embed-text-v1.5 (Q4_0)

Mac M2

- LLM: Qwen2.5 7B Instruct (Q4_K_M) or Llama 3.1 8B Instruct (Q4_K_M)
- Embeddings: bge-base-en-v1.5 (Q4_0) or nomic-embed-text-v1.5 (Q4_0)

Mac M3

- LLM: Qwen2.5 14B Instruct (Q4_K_M) or Llama 3.1 8B Instruct (Q4_K_M)
- Embeddings: bge-base-en-v1.5 (Q4_0) or nomic-embed-text-v1.5 (Q4_0)

Mac M4

- LLM: Qwen2.5 14B Instruct (Q4_K_M) or Llama 3.1 8B Instruct (Q4_K_M)
- Embeddings: bge-base-en-v1.5 (Q4_0) or nomic-embed-text-v1.5 (Q4_0)

# Tree

```
HydroWise/
â”œâ”€ apps/
â”‚  â”œâ”€ web/                              # React (Vite) UI (web + desktop webview)
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â”‚  â”œâ”€ components/                 # chat UI, sidebar, inputs, modals
â”‚  â”‚  â”‚  â”œâ”€ pages/                      # chat, courses, docs, notes, quizzes, analytics
â”‚  â”‚  â”‚  â”œâ”€ hooks/                      # LLM init, chat orchestration
â”‚  â”‚  â”‚  â”œâ”€ store/                      # Zustand stores (chat/history/etc)
â”‚  â”‚  â”‚  â”œâ”€ theme.ts                    # MUI theme
â”‚  â”‚  â”‚  â””â”€ main.tsx
â”‚  â”‚  â”œâ”€ index.html
â”‚  â”‚  â”œâ”€ vite.config.ts
â”‚  â”‚  â””â”€ package.json
â”‚  â”‚
â”‚  â””â”€ desktop/                          # Tauri wrapper
â”‚     â”œâ”€ src-tauri/
â”‚     â”‚  â”œâ”€ src/                        # sidecar spawn, lifecycle, ports
â”‚     â”‚  â”œâ”€ tauri.conf.json
â”‚     â”‚  â””â”€ Cargo.toml
â”‚     â””â”€ package.json
â”‚
â”œâ”€ services/
â”‚  â””â”€ api/                              # Hono API (web + local desktop)
â”‚     â”œâ”€ src/
â”‚     â”‚  â”œâ”€ routes/                     # auth, chat, docs, courses, notes, quizzes
â”‚     â”‚  â”œâ”€ db/                         # Drizzle client wrapper (PG/PGlite)
â”‚     â”‚  â”œâ”€ llm/                        # llama.cpp integration (desktop)
â”‚     â”‚  â”œâ”€ config.ts                   # MODE=web|desktop
â”‚     â”‚  â””â”€ server.ts
â”‚     â””â”€ package.json
â”‚
â”œâ”€ packages/
â”‚  â”œâ”€ db/                               # Drizzle schema + clients
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â”œâ”€ drizzle.config.ts
â”‚  â”‚  â””â”€ package.json
â”‚  â”œâ”€ core/                             # parsing, chunking, RAG helpers
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â””â”€ package.json
â”‚  â”œâ”€ contracts/                        # shared Zod + TS API schemas
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â””â”€ package.json
â”‚  â”œâ”€ entities/                         # shared entities (Chat, Message, etc)
â”‚  â”‚  â”œâ”€ src/
â”‚  â”‚  â””â”€ package.json
â”‚  â””â”€ llm-client/                       # AI SDK integration (WebLLM + OpenAI-compatible)
â”‚     â”œâ”€ src/
â”‚     â”‚  â”œâ”€ web/                        # @browser-ai/web-llm provider
â”‚     â”‚  â”œâ”€ desktop/                    # @ai-sdk/openai (llama-server endpoint)
â”‚     â”‚  â””â”€ client.ts
â”‚     â””â”€ package.json
â”‚
â”œâ”€ .github/
â”‚  â””â”€ workflows/                        # desktop release workflow
â”œâ”€ package.json                         # workspace root (bun)
â”œâ”€ biome.json                           # lint/format config
â””â”€ ROADMAP.md
```

# Database Schema (Drizzle)

**users**:
- id (primary key)
- email (unique)
- created_at

**chats**:
- id (primary key)
- user_id (foreign key -> users, indexed)
- name
- created_at

**messages**:
- id (primary key)
- chat_id (indexed)
- role (enum: user | assistant)
- content
- created_at
</file>

<file path="apps/web/src/components/HydroChat.tsx">
import type { Message } from "@hydrowise/entities";
import { Box, Chip, Paper, Stack } from "@mui/material";
import { useMemo } from "react";
import { HydroInputContainer } from "@/components/ui/HydroInputContainer";
import { HydroMessage } from "@/components/ui/HydroMessage";
import { useChat } from "@/hooks/useChat";
import { useWebLLMEngine } from "@/hooks/useInitEngine";

export const HydroChat = () => {
  const { messages, submitMessage, isStreaming, streamingContent } = useChat();
  const { engineReady, loadProgress } = useWebLLMEngine();

  const displayMessages = useMemo((): Message[] => {
    const baseMessages = messages ?? [];
    if (!isStreaming || streamingContent === null) return baseMessages;
    return [
      ...baseMessages,
      { role: "assistant" as const, content: streamingContent },
    ];
  }, [messages, isStreaming, streamingContent]);

  const handleSend = async (content: string) => {
    if (!engineReady) return;
    await submitMessage(content);
  };

  return (
    <Box
      sx={{
        display: "flex",
        flexDirection: "column",
        height: "100%",
        maxWidth: 840,
        mx: "auto",
        px: { xs: 2, sm: 3 },
        py: { xs: 2, sm: 3 },
        minWidth: 0,
      }}
    >
      <Paper
        elevation={0}
        sx={{
          display: "flex",
          flexDirection: "column",
          flex: 1,
          borderRadius: 3,
          border: "1px solid",
          borderColor: "divider",
          bgcolor: "background.paper",
          p: { xs: 2, sm: 3 },
          boxShadow: "0 10px 30px rgba(0, 0, 0, 0.06)",
          minHeight: 0,
        }}
      >
        <Stack
          direction={{ xs: "column", sm: "row" }}
          spacing={1.5}
          alignItems="center"
          sx={{ mb: 2 }}
        >
          <Chip
            label={engineReady ? "Ready" : "Warming up"}
            color={engineReady ? "secondary" : "default"}
            variant={engineReady ? "filled" : "outlined"}
            size="small"
          />
        </Stack>

        <Stack
          spacing={1.5}
          sx={{
            flex: 1,
            overflowY: "auto",
            minHeight: 0,
            pr: 0.5,
          }}
        >
          {displayMessages.map((entry: Message, index: number) => (
            <HydroMessage key={`${entry.role}-${index}`} message={entry} />
          ))}
        </Stack>
        <Box sx={{ mt: 2 }}>
          <HydroInputContainer
            onSend={handleSend}
            disabled={!engineReady}
            loadingProgress={loadProgress}
          />
        </Box>
      </Paper>
    </Box>
  );
};
</file>

</files>
